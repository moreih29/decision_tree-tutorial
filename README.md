# Decision Tree tutorial

## 개요
![결정 트리 기본 구조](https://tensorflowkorea.files.wordpress.com/2017/06/2-22.png?w=768&h=546)
- Decision Tree(결정 트리)는 분류와 회귀 문제에서 널리 사용되는 모델
- 기본적으로 최종 결정에 다다르기 위해 그림과 같이 예/아니오 질문을 이어나가면서 학습
- 질문의 내용을 명확하게 확인할 수 있기 때문에, 모델의 결정을 설명할 수 있음

## 동작 방식
![결정 트리 설명을 위한 데이터의 분포](https://tensorflowkorea.files.wordpress.com/2017/06/2-23.png?w=768)
- 결정 트리의 동작 방식을 설명하기 위한 예제 데이터의 분포
- 두 개의 클래스를 가지고 있으며, 각 클래스는 반대 되는 반달 모양의 분포를 갖고 있음 
- 개요의 예제와 달리 연속적인 데이터에 적용할 때에는 "특성 i는 값 a보다 큰가?"와 같은 기준으로 나뉘어짐

### Step 1
![결정 트리 동작 1](https://tensorflowkorea.files.wordpress.com/2017/06/2-24.png?w=768)
- 첫 번째 분기는 x[1]=0.0596에서 수평으로 전체 데이터를 나눔
- 루트 노드는 각 클래스 별로 50개 씩 포함된 [50, 50]의 상태
- 해당 조건 분기를 통해 [2, 32]의 상태를 갖는 하위 노드와 [48, 18]의 상태를 갖는 하위 노드로 나눌 수 있음

### Step 2
![결정 트리 동작 2](https://tensorflowkorea.files.wordpress.com/2017/06/2-25.png?w=768)
- 위의 프로세스를 반복하여, 두 클래스를 잘 나눌 수 있는 조건을 생성
- 각 조건은 하나의 특성에 대해서만 이루어지기 때문에 나누어진 영역은 항상 축에 평행

### Step 3
![결정 트리 동작 3](https://tensorflowkorea.files.wordpress.com/2017/06/2-26.png?w=768)
- 분기를 지속적으로 반복하여 모든 데이터 포인트에 대해 클래스를 나눌 수 있는 분기를 탐색함
- 최종적으로 분할된 영역(노드)를 리프 노드라고 함
- 분할된 영역에 타겟 값이 하나만 포함된 경우를 순수 노드(pure node)라고 함
- 예측 과정의 경우, 새로운 데이터를 넣어 해당 조건들에 맞게 분기를 따라간 영역의 클래스로 분류
- 회귀 문제의 경우도 동일하게 진행, 출력 값은 리프 노드의 평균 값

## 복잡도 제어
- 일반적으로 모든 리프 노드가 순수 노드가 될 때까지 분기를 반복하면, 모델의 깊이가 깊어지고 훈련 데이터에 과적합
- 과적합이 될 경우, 훈련 데이터에 포함되지 않은 테스트 데이터는 성능이 낮을 수 있음
- 과적합을 막는 전략은, 트리 생성을 일찍 중단하는 사전 가지치기(pre-pruning)과 사후 가지치기(post-pruning)이 있음
- scikit-learn의 경우 사전 가지치기만 지원함

---
출처
- https://tensorflow.blog/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2-3-5-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%AC/